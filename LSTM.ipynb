{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_u2fFfgCQ7_e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w-uVxiRQm1jx"
      },
      "outputs": [],
      "source": [
        "# @title Extra Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - x ** 2\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P_23liUmRZox"
      },
      "outputs": [],
      "source": [
        "class RecurrentNeuralNetwork:\n",
        "    def __init__ (self, input, output, recurrences, expected_output, learning_rate):\n",
        "        #initial input \n",
        "        self.x = np.zeros(input)\n",
        "        #input size \n",
        "        self.input = input\n",
        "        #expected output \n",
        "        self.y = np.zeros(output)\n",
        "        #output size\n",
        "        self.output = output\n",
        "        #weight matrix \n",
        "        self.w = np.random.random((output, output))\n",
        "        #matrix used in RMSprop in order to decay the learning rate\n",
        "        self.G = np.zeros_like(self.w)\n",
        "        #length of the recurrent network\n",
        "        self.recurrences = recurrences\n",
        "        #learning rate \n",
        "        self.learning_rate = learning_rate\n",
        "        #array for storing inputs\n",
        "        self.ia = np.zeros((recurrences+1,input))\n",
        "        #array for storing cell states\n",
        "        self.ca = np.zeros((recurrences+1,output))\n",
        "        #array for storing outputs\n",
        "        self.oa = np.zeros((recurrences+1,output))\n",
        "        #array for storing hidden states\n",
        "        self.ha = np.zeros((recurrences+1,output))\n",
        "        #forget gate \n",
        "        self.af = np.zeros((recurrences+1,output))\n",
        "        #input gate\n",
        "        self.ai = np.zeros((recurrences+1,output))\n",
        "        #cell state\n",
        "        self.ac = np.zeros((recurrences+1,output))\n",
        "        #output gate\n",
        "        self.ao = np.zeros((recurrences+1,output))\n",
        "        #array of expected output values\n",
        "        self.expected_output = np.vstack((np.zeros(expected_output.shape[0]), expected_output.T))\n",
        "        #declare LSTM cell \n",
        "        self.LSTM = LSTM(input, output, recurrences, learning_rate)\n",
        "    \n",
        "    #sigmoid activation function\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    #derivative of sigmoid \n",
        "    def dsigmoid(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))  \n",
        "    \n",
        "    #Forward Propagation\n",
        "    def forwardProp(self):\n",
        "        for i in range(1, self.recurrences+1):\n",
        "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
        "            cs, hs, f, c, o = self.LSTM.forwardProp()\n",
        "            #store cell state from the forward propagation\n",
        "            self.ca[i] = cs #cell state\n",
        "            self.ha[i] = hs #hidden state\n",
        "            self.af[i] = f #forget state\n",
        "            self.ai[i] = inp #inpute gate\n",
        "            self.ac[i] = c #cell state\n",
        "            self.ao[i] = o #output gate\n",
        "            self.oa[i] = self.sigmoid(np.dot(self.w, hs)) #activate the weight*input\n",
        "            self.x = self.expected_output[i-1]\n",
        "        return self.oa\n",
        "\n",
        "    # Back propagation\n",
        "def backProp(self):\n",
        "        totalError = 0\n",
        "        #cell state\n",
        "        dfcs = np.zeros(self.output)\n",
        "        #hidden state,\n",
        "        dfhs = np.zeros(self.output)\n",
        "        #weight matrix\n",
        "        tu = np.zeros((self.output,self.output))\n",
        "        #forget gate\n",
        "        tfu = np.zeros((self.output, self.input+self.output))\n",
        "        #input gate\n",
        "        tiu = np.zeros((self.output, self.input+self.output))\n",
        "        #cell unit\n",
        "        tcu = np.zeros((self.output, self.input+self.output))\n",
        "        #output gate\n",
        "        tou = np.zeros((self.output, self.input+self.output))\n",
        "        for i in range(self.recurrences, -1, -1):\n",
        "            error = self.oa[i] - self.expected_output[i]\n",
        "            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n",
        "            error = np.dot(error, self.w)\n",
        "            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n",
        "            self.LSTM.cs = self.ca[i]\n",
        "            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n",
        "            totalError += np.sum(error)\n",
        "            #forget gate\n",
        "            tfu += fu\n",
        "            #input gate\n",
        "            tiu += iu\n",
        "            #cell state\n",
        "            tcu += cu\n",
        "            #output gate\n",
        "            tou += ou   \n",
        "        self.LSTM.update(tfu/self.recurrences, tiu/self.recurrences, tcu/self.recurrences, tou/self.recurrences)  \n",
        "        self.update(tu/self.recurrences)\n",
        "        return totalError\n",
        "    \n",
        "    def update(self, u):\n",
        "        self.G = 0.95 * self.G + 0.1 * u**2  \n",
        "        self.w -= self.learning_rate/np.sqrt(self.G + 1e-8) * u\n",
        "        return\n",
        "    \n",
        "    def sample(self):\n",
        "        for i in range(1, self.recurrences+1):\n",
        "            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n",
        "            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n",
        "            maxI = np.argmax(self.x)\n",
        "            self.x = np.zeros_like(self.x)\n",
        "            self.x[maxI] = 1\n",
        "            self.ia[i] = self.x \n",
        "            #store cell states\n",
        "            self.ca[i] = cs\n",
        "            #store hidden state\n",
        "            self.ha[i] = hs\n",
        "            #forget gate\n",
        "            self.af[i] = f\n",
        "            #input gate\n",
        "            self.ai[i] = inp\n",
        "            #cell state\n",
        "            self.ac[i] = c\n",
        "            #output gate\n",
        "            self.ao[i] = o\n",
        "            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n",
        "            maxI = np.argmax(self.oa[i])\n",
        "            newX = np.zeros_like(self.x)\n",
        "            newX[maxI] = 1\n",
        "            self.x = newX\n",
        "        return self.oa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b87YY4BTt_kD"
      },
      "outputs": [],
      "source": [
        "# @title LSTM\n",
        "class LSTMCell:\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Initialize weights for the input gate\n",
        "        self.W_i = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
        "        self.b_i = np.zeros((1, hidden_size))\n",
        "\n",
        "        # Initialize weights for the forget gate\n",
        "        self.W_f = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
        "        self.b_f = np.zeros((1, hidden_size))\n",
        "\n",
        "        # Initialize weights for the output gate\n",
        "        self.W_o = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
        "        self.b_o = np.zeros((1, hidden_size))\n",
        "\n",
        "        # Initialize weights for the cell state\n",
        "        self.W_c = np.random.randn(input_size + hidden_size, hidden_size) * 0.1\n",
        "        self.b_c = np.zeros((1, hidden_size))\n",
        "\n",
        "        # Initialize the internal states to zero\n",
        "        self.h_prev = np.zeros((1, hidden_size))\n",
        "        self.C_prev = np.zeros((1, hidden_size))\n",
        "\n",
        "        self.current_weights = [self.W_i, self.b_i, \n",
        "                                self.W_f, self.b_f,\n",
        "                                self.W_o, self.b_o,\n",
        "                                self.W_c, self.b_c]\n",
        "\n",
        "    def concatenate_input_hidden(self, input_t):\n",
        "        # Ensure input_t is a 2D array with shape (1, input_size)\n",
        "        input_t = input_t.reshape(1, -1)  # Reshaping to make it 2D\n",
        "        # Concatenate the current input with the previous hidden state\n",
        "        combined = np.hstack((input_t, self.h_prev))\n",
        "        return combined\n",
        "\n",
        "    def forward_step(self, input_t):\n",
        "        combined = self.concatenate_input_hidden(input_t)\n",
        "        # Compute gate activations\n",
        "        self.i_t = sigmoid(np.dot(combined, self.W_i) + self.b_i)\n",
        "        self.f_t = sigmoid(np.dot(combined, self.W_f) + self.b_f)\n",
        "        self.o_t = sigmoid(np.dot(combined, self.W_o) + self.b_o)\n",
        "        self.g_t = tanh(np.dot(combined, self.W_c) + self.b_c)\n",
        "        # Update cell state\n",
        "        self.C_t = self.f_t * self.C_prev + self.i_t * self.g_t\n",
        "        # Compute hidden state\n",
        "        C_tanh = tanh(self.C_t)\n",
        "        self.h_t = self.o_t * C_tanh\n",
        "        # Store intermediates for backward pass\n",
        "        self.cache = (combined, self.i_t, self.f_t, self.o_t, self.g_t, self.C_prev, C_tanh)\n",
        "        # Update previous states\n",
        "        self.h_prev = self.h_t\n",
        "        self.C_prev = self.C_t\n",
        "        return self.h_t, self.C_t\n",
        "\n",
        "    def forward_sequence(self, input_sequence):\n",
        "        # Assuming input_sequence is shaped (time_steps, input_size)\n",
        "        # Initialize outputs\n",
        "        h_sequence = np.zeros((input_sequence.shape[0], self.hidden_size))\n",
        "        C_sequence = np.zeros((input_sequence.shape[0], self.hidden_size))\n",
        "\n",
        "        for t in range(input_sequence.shape[0]):\n",
        "            h_t, C_t = self.forward_step(input_sequence[t])\n",
        "            h_sequence[t] = h_t\n",
        "            C_sequence[t] = C_t\n",
        "\n",
        "        return h_sequence, C_sequence\n",
        "\n",
        "    def compute_loss_and_gradient(self, y_true, y_pred, loss_type='MSE', gradient=False):\n",
        "        \"\"\" \n",
        "        Compute the loss and its gradient.\n",
        "\n",
        "        Parameters:\n",
        "        - y_true: np.array, true labels or target values\n",
        "        - y_pred: np.array, predicted labels or values from the last layer of the network\n",
        "        - loss_type: str, type of the loss function ('MSE' for mean squared error or 'CE' for cross-entropy)\n",
        "\n",
        "        Returns:\n",
        "        - loss: float, the computed loss value\n",
        "        - grad_loss: np.array, gradient of the loss with respect to y_pred\n",
        "        \"\"\"\n",
        "        if loss_type == 'MSE':\n",
        "            # Compute MSE loss\n",
        "            loss = np.mean((y_true - y_pred) ** 2)\n",
        "            # Compute gradient w.r.t. y_pred for MSE\n",
        "            if gradient:\n",
        "                grad_loss = -2 * (y_true - y_pred) / y_true.size\n",
        "        elif loss_type == 'CE':\n",
        "            # Compute Cross-Entropy loss\n",
        "            # Assuming y_true is one-hot encoded for classification tasks\n",
        "            loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]\n",
        "            # Compute gradient w.r.t. y_pred for CE\n",
        "            if gradient:\n",
        "                grad_loss = -(y_true / (y_pred + 1e-9)) / y_true.shape[0]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid loss type specified. Choose 'MSE' or 'CE'.\")\n",
        "        if gradient:\n",
        "            return loss, grad_loss\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "    def bakward_stochastic(self, input_t, y_true, learning_rate=0.01, h_range=(1e-5, 1e-2), loss_type='MSE'):\n",
        "        # Input Gate Loss\n",
        "        y_pred, _ = self.forward_step(input_t)\n",
        "        h_W = np.random.uniform(h_range[0], h_range[1], self.W_i.shape)\n",
        "        h_b = np.random.uniform(h_range[0], h_range[1], self.b_i.shape)\n",
        "        \n",
        "        original_loss = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "\n",
        "        original_W = self.W_i.copy()\n",
        "        original_b = self.b_i.copy()\n",
        "        self.W_i = self.W_i + h_W\n",
        "        d_loss_W = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.W_i = original_W\n",
        "\n",
        "        self.b_i = self.b_i + h_b\n",
        "        d_loss_b = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.b_i = original_b\n",
        "\n",
        "        dW_i = (d_loss_W - original_loss) / h_W\n",
        "        db_i = (d_loss_b - original_loss) / h_b\n",
        "        self.W_i -= learning_rate * dW_i\n",
        "        self.b_i -= learning_rate * db_i\n",
        "\n",
        "        # Forget Gate Loss\n",
        "        h_W = np.random.uniform(h_range[0], h_range[1], self.W_f.shape)\n",
        "        h_b = np.random.uniform(h_range[0], h_range[1], self.b_f.shape)\n",
        "        \n",
        "        original_W = self.W_f.copy()\n",
        "        original_b = self.b_f.copy()\n",
        "        self.W_f = self.W_f + h_W\n",
        "        d_loss_W = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.W_f = original_W\n",
        "\n",
        "        self.b_f = self.b_f + h_b\n",
        "        d_loss_b = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.b_f = original_b\n",
        "\n",
        "        dW_f = (d_loss_W - original_loss) / h_W\n",
        "        db_f = (d_loss_b - original_loss) / h_b\n",
        "        self.W_f -= learning_rate * dW_f\n",
        "        self.b_f -= learning_rate * db_f\n",
        "\n",
        "        # Output Gate Loss\n",
        "        h_W = np.random.uniform(h_range[0], h_range[1], self.W_o.shape)\n",
        "        h_b = np.random.uniform(h_range[0], h_range[1], self.b_o.shape)\n",
        "        \n",
        "        original_W = self.W_o.copy()\n",
        "        original_b = self.b_o.copy()\n",
        "        self.W_o = self.W_o + h_W\n",
        "        d_loss_W = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.W_o = original_W\n",
        "\n",
        "        self.b_o = self.b_o + h_b\n",
        "        d_loss_b = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.b_o = original_b\n",
        "\n",
        "        dW_o = (d_loss_W - original_loss) / h_W\n",
        "        db_o = (d_loss_b - original_loss) / h_b\n",
        "        self.W_o -= learning_rate * dW_o\n",
        "        self.b_o -= learning_rate * db_o\n",
        "\n",
        "        # Cell Gate Loss\n",
        "        h_W = np.random.uniform(h_range[0], h_range[1], self.W_c.shape)\n",
        "        h_b = np.random.uniform(h_range[0], h_range[1], self.b_c.shape)\n",
        "        \n",
        "        original_loss = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "\n",
        "        original_W = self.W_c.copy()\n",
        "        original_b = self.b_c.copy()\n",
        "        self.W_c = self.W_c + h_W\n",
        "        d_loss_W = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.W_c = original_W\n",
        "\n",
        "        self.b_c = self.b_c + h_b\n",
        "        d_loss_b = self.compute_loss_and_gradient(y_true, y_pred, loss_type)\n",
        "        self.b_c = original_b\n",
        "\n",
        "        dW_c = (d_loss_W - original_loss) / h_W\n",
        "        db_c = (d_loss_b - original_loss) / h_b\n",
        "        self.W_c -= learning_rate * dW_c\n",
        "        self.b_c -= learning_rate * db_c\n",
        "\n",
        "        return dW_i, db_i, dW_f, db_f, dW_c, db_c, dW_o, db_o\n",
        "\n",
        "    def backward_step(self, dh, dC, cache):\n",
        "        combined, i, f, o, g, C_prev, C_tanh = cache\n",
        "        # Derivatives of the activation functions\n",
        "        do = dh * C_tanh * sigmoid_derivative(o)\n",
        "        dC_tanh = dh * o * tanh_derivative(C_tanh)\n",
        "        dC += dC_tanh\n",
        "        di = dC * g * sigmoid_derivative(i)\n",
        "        dg = dC * i * tanh_derivative(g)\n",
        "        df = dC * C_prev * sigmoid_derivative(f)\n",
        "        dC_prev = dC * f\n",
        "        \n",
        "        dcombined_i = np.dot(di, self.W_i.T)\n",
        "        dcombined_f = np.dot(df, self.W_f.T)\n",
        "        dcombined_o = np.dot(do, self.W_o.T)\n",
        "        dcombined_g = np.dot(dg, self.W_c.T)\n",
        "        dcombined = dcombined_i + dcombined_f + dcombined_o + dcombined_g\n",
        "        \n",
        "        # Gradients with respect to parameters\n",
        "        dW_i = np.dot(combined.T, di)\n",
        "        dW_f = np.dot(combined.T, df)\n",
        "        dW_o = np.dot(combined.T, do)\n",
        "        dW_c = np.dot(combined.T, dg)\n",
        "        \n",
        "        db_i = np.sum(di, axis=0, keepdims=True)\n",
        "        db_f = np.sum(df, axis=0, keepdims=True)\n",
        "        db_o = np.sum(do, axis=0, keepdims=True)\n",
        "        db_c = np.sum(dg, axis=0, keepdims=True)\n",
        "        \n",
        "        # Update LSTM parameters (shown separately in the next step)\n",
        "        return dW_i, dW_f, dW_o, dW_c, db_i, db_f, db_o, db_c, dcombined[:, :self.input_size], dC_prev\n",
        "\n",
        "    def backward_sequence(self, input_sequence, dh_sequence, learning_rate=0.01):\n",
        "        dC_next = np.zeros((1, self.hidden_size))\n",
        "        dh_next = np.zeros((1, self.hidden_size))\n",
        "        gradients = np.zeros_like(self.W_i)  # Placeholder for accumulated gradients\n",
        "        \n",
        "        for t in reversed(range(len(input_sequence))):\n",
        "            dh = dh_sequence[t] + dh_next\n",
        "            cache = self.cache[t]  # Assuming you've stored each timestep's cache during the forward pass\n",
        "            grad_values = self.backward_step(dh, dC_next, cache)\n",
        "            dW_i, dW_f, dW_o, dW_c, db_i, db_f, db_o, db_c, dh_next, dC_next = grad_values\n",
        "            \n",
        "            # Accumulate gradients from all timesteps\n",
        "            gradients += np.array([dW_i, dW_f, dW_o, dW_c, db_i, db_f, db_o, db_c])\n",
        "            \n",
        "            # Update parameters here or after accumulating all gradients\n",
        "            \n",
        "        # Example parameter update after accumulating gradients\n",
        "        self.W_i -= learning_rate * gradients[0]\n",
        "        self.W_f -= learning_rate * gradients[1]\n",
        "        self.W_o -= learning_rate * gradients[2]\n",
        "        self.W_c -= learning_rate * gradients[3]\n",
        "        self.b_i -= learning_rate * gradients[4]\n",
        "        self.b_f -= learning_rate * gradients[5]\n",
        "        self.b_o -= learning_rate * gradients[6]\n",
        "        self.b_c -= learning_rate * gradients[7]\n",
        "\n",
        "    def train_on_loss(self, input_sequence, pseudo_loss_grad, learning_rate=0.01):\n",
        "        # Assuming pseudo_loss_grad is a gradient of loss w.r.t. LSTM output\n",
        "        # Forward pass through the sequence\n",
        "        self.forward_sequence(input_sequence)\n",
        "\n",
        "        # Create a dummy gradient sequence based on pseudo_loss_grad\n",
        "        # Assuming the loss gradient is the same for each timestep for simplification\n",
        "        dh_sequence = np.array([pseudo_loss_grad] * len(input_sequence))\n",
        "        dC_sequence = np.zeros_like(dh_sequence)  # Assuming no direct gradient for cell state\n",
        "\n",
        "        # Perform backward pass and update parameters\n",
        "        self.backward_sequence(dh_sequence, dC_sequence, learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LSTM input and model parameters\n",
        "input_size = 3  # Size of input vector\n",
        "hidden_size = 4  # Size of LSTM's hidden state\n",
        "time_steps = 5  # Length of the sequence\n",
        "\n",
        "# Generate a random sequence of inputs\n",
        "input_sequence = np.random.rand(time_steps, input_size)\n",
        "\n",
        "# Define a simple loss derivative as a placeholder\n",
        "# In practice, this would come from the derivative of the loss function\n",
        "# Assuming the loss is with respect to the hidden state of the last timestep\n",
        "dh_sequence = np.zeros((time_steps, hidden_size))\n",
        "dh_sequence[-1] = np.array([0.5] * hidden_size)  # Simple gradient from loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the LSTM model\n",
        "lstm_model = LSTMCell(input_size, hidden_size)\n",
        "\n",
        "# Perform the forward pass\n",
        "h_sequence, _ = lstm_model.forward_sequence(input_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. ],\n",
              "       [0. , 0. , 0. , 0. ],\n",
              "       [0.5, 0.5, 0.5, 0.5]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dh_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 7, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dC_sequence \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(dh_sequence)  \u001b[38;5;66;03m# Assuming no direct loss from cell states for simplicity\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Backward pass through the sequence\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdh_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Normally, you'd compute the actual gradients here based on the loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# For this example, we directly use dh_sequence as a placeholder for loss gradients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Update LSTM parameters (assuming the method is correctly implemented within LSTMCell)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m  \u001b[38;5;66;03m# Placeholder learning rate\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 106\u001b[0m, in \u001b[0;36mLSTMCell.backward_sequence\u001b[0;34m(self, input_sequence, dh_sequence, learning_rate)\u001b[0m\n\u001b[1;32m    104\u001b[0m dh \u001b[38;5;241m=\u001b[39m dh_sequence[t] \u001b[38;5;241m+\u001b[39m dh_next\n\u001b[1;32m    105\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[t]  \u001b[38;5;66;03m# Assuming you've stored each timestep's cache during the forward pass\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m grad_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdC_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m dW_i, dW_f, dW_o, dW_c, db_i, db_f, db_o, db_c, dh_next, dC_next \u001b[38;5;241m=\u001b[39m grad_values\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Accumulate gradients from all timesteps\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36mLSTMCell.backward_step\u001b[0;34m(self, dh, dC, cache)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dh, dC, cache):\n\u001b[0;32m---> 68\u001b[0m     combined, i, f, o, g, C_prev, C_tanh \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Derivatives of the activation functions\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     do \u001b[38;5;241m=\u001b[39m dh \u001b[38;5;241m*\u001b[39m C_tanh \u001b[38;5;241m*\u001b[39m sigmoid_derivative(o)\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 1)"
          ]
        }
      ],
      "source": [
        "# Placeholder for simplicity; in practice, use actual loss derivatives\n",
        "dC_sequence = np.zeros_like(dh_sequence)  # Assuming no direct loss from cell states for simplicity\n",
        "\n",
        "# Backward pass through the sequence\n",
        "gradients = lstm_model.backward_sequence(input_sequence, dh_sequence)\n",
        "\n",
        "# Normally, you'd compute the actual gradients here based on the loss\n",
        "# For this example, we directly use dh_sequence as a placeholder for loss gradients\n",
        "\n",
        "# Update LSTM parameters (assuming the method is correctly implemented within LSTMCell)\n",
        "learning_rate = 0.01  # Placeholder learning rate\n",
        "lstm_model.update_parameters(gradients, learning_rate=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
